use axum::{extract::{Path, State, Extension}, Json};
use std::collections::HashMap;
use std::sync::atomic::Ordering;
use std::time::Instant;
use crate::{Metric, Document};
use crate::error::{Result, ServerError};
use crate::server::metrics::{record_lock_read, record_lock_write};
use super::super::{
    state::SharedState,
    types::*,
    helpers::{json_to_metadata, metadata_to_json},
};
use tracing::info;

// Embedding endpoints: embed text then reuse storage/search flows

// Parse similarity metric from string
fn parse_metric(s: Option<String>) -> Metric {
    match s.as_deref() {
        Some("euclidean") => Metric::Euclidean,
        Some("dot") | Some("dot_product") => Metric::DotProduct,
        _ => Metric::Cosine,
    }
}

// POST /api/collections/:collection/embed - embed text and store
pub async fn embed_text(
    State(state): State<SharedState>,
    Path(collection): Path<String>,
    Json(req): Json<EmbedRequest>,
) -> Result<Json<EmbedResultsResponse>> {
    if state.shutting_down.load(Ordering::Relaxed) {
        return Err(ServerError::ServiceUnavailable("Server is shutting down".to_string()).into());
    }
    state.ensure_write_allowed()?;

    state.get_or_create_collection(&collection)?;

    let embedder = state.embedder.as_ref()
        .ok_or(ServerError::ServiceUnavailable(super::super::helpers::EMBEDDING_NOT_CONFIGURED.to_string()))?;

    let response = match (req.text.clone(), req.texts.clone()) {
        (Some(text), None) => {
            info!(collection=%collection, "embed_single_request");
            let start = Instant::now();
            let response = embedder.embed(&text).await?;
            let embed_duration = start.elapsed();

            let storage_ref = state.collections.get(&collection)
                .ok_or_else(|| ServerError::NotFound(super::super::helpers::COLLECTION_NOT_FOUND.to_string()))?;
            let lock_start = Instant::now();
            let mut storage = storage_ref.write();
            record_lock_write(state.latency_tracker.get(&collection).as_deref(), lock_start);

            let metadata = json_to_metadata(req.metadata);
            let entry = Document::with_metadata(
                response.embedding.clone(),
                text,
                metadata,
            );

            let id = storage.insert(entry)?;
            state.enforce_cache_budget();
            state.embed_metrics.record(1, 1, response.tokens.unwrap_or(0) as u64, embed_duration);

            EmbedResultsResponse::Single(EmbedResponse {
                id: id.to_string(),
                embedding: response.embedding,
                tokens: response.tokens,
            })
        }
        (None, Some(texts)) => {
            if texts.is_empty() {
                return Err(ServerError::InvalidRequest("texts cannot be empty".to_string()).into());
            }
            info!(collection=%collection, batch=texts.len(), "embed_batch_request");

            let mut ids = Vec::with_capacity(texts.len());
            let mut embeddings = Vec::with_capacity(texts.len());
            let mut total_tokens: u32 = 0;
            let mut entries = Vec::with_capacity(texts.len());
            let start = Instant::now();
            for (idx, t) in texts.iter().enumerate() {
                let resp = embedder.embed(t).await?;
                embeddings.push(resp.embedding.clone());
                if let Some(tokens) = resp.tokens {
                    total_tokens = total_tokens.saturating_add(tokens);
                }
                let md = if idx < req.metadata_list.len() {
                    json_to_metadata(req.metadata_list[idx].clone())
                } else {
                    json_to_metadata(HashMap::new())
                };
                entries.push(Document::with_metadata(resp.embedding, t.clone(), md));
            }

            let storage_ref = state.collections.get(&collection)
                .ok_or_else(|| ServerError::NotFound(super::super::helpers::COLLECTION_NOT_FOUND.to_string()))?;
            let lock_start = Instant::now();
            let mut storage = storage_ref.write();
            record_lock_write(state.latency_tracker.get(&collection).as_deref(), lock_start);

            let insert_ids = storage.insert_batch(entries)?;
            ids.extend(insert_ids.into_iter().map(|id| id.to_string()));
            state.enforce_cache_budget();
            state.embed_metrics.record(1, ids.len() as u64, total_tokens as u64, start.elapsed());

            EmbedResultsResponse::Multi(MultiEmbedResponse {
                ids,
                embeddings,
                total_tokens: if total_tokens > 0 { Some(total_tokens) } else { None },
            })
        }
        (Some(_), Some(_)) => return Err(ServerError::InvalidRequest("Provide either text or texts, not both".to_string()).into()),
        (None, None) => return Err(ServerError::InvalidRequest("No text provided".to_string()).into()),
    };

    Ok(Json(response))
}

// POST /api/collections/:collection/search/text - search by text query
pub async fn search_by_text(
    State(state): State<SharedState>,
    Path(collection): Path<String>,
    Extension(request_id): Extension<crate::server::request_id::RequestId>,
    Json(req): Json<TextSearchRequest>,
) -> Result<Json<SearchResponse>> {
    if state.shutting_down.load(Ordering::Relaxed) {
        return Err(ServerError::ServiceUnavailable("Server is shutting down".to_string()).into());
    }

    state.get_or_create_collection(&collection)?;

    let embedder = state.embedder.as_ref()
        .ok_or(ServerError::ServiceUnavailable(super::super::helpers::EMBEDDING_NOT_CONFIGURED.to_string()))?;

    info!(collection=%collection, "search_by_text_request");
    let start = Instant::now();
    let response = embedder.embed(&req.query).await?;
    let embed_duration = start.elapsed();
    state.embed_metrics.record(1, 1, response.tokens.unwrap_or(0) as u64, embed_duration);

    let metric = parse_metric(req.metric);
    let effective_search = crate::server::handlers::vectors::apply_search_overrides(
        state.collections.get(&collection)
            .map(|c| c.read().config().search)
            .unwrap_or_default(),
        req.ef,
        req.nprobe,
        req.overfetch,
        req.preset.clone(),
    );

    let storage_ref = state.collections.get(&collection)
        .ok_or_else(|| ServerError::NotFound(super::super::helpers::COLLECTION_NOT_FOUND.to_string()))?;
    let lock_start = Instant::now();
    let storage = storage_ref.read();
    record_lock_read(state.latency_tracker.get(&collection).as_deref(), lock_start);

    let start = Instant::now();
    let results: Vec<HitResponse> = storage
        .search(
            &response.embedding,
            req.k,
            metric,
            crate::SearchParams {
                mode: storage.config().execution,
                filter: None,
                filter_overfetch_override: req.overfetch,
                search_config_override: Some(effective_search),
            },
        )
        .into_iter()
        .map(|r| HitResponse {
            id: r.id.to_string(),
            score: r.score,
            text: r.text,
            metadata: metadata_to_json(&r.metadata),
        })
        .collect();
    let duration = start.elapsed();
    if duration.as_millis() > state.slow_query_ms {
        tracing::warn!(
            collection=%collection,
            request_id = request_id.0.as_str(),
            elapsed_ms = duration.as_millis(),
            "slow_text_search"
        );
    }
    
    // Record latency
    if let Some(tracker) = state.latency_tracker.get(&collection) {
        tracker.record_search(duration);
    }

    Ok(Json(SearchResponse { 
        results,
        latency_ms: Some(duration.as_millis() as f32),
    }))
}
